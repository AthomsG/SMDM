for (column in 1:length(data_norm)){
if (length(unique(data_norm[[column]])) == 2){
print('test')
# data_binary$colnames(data_norm[column]) <- data_norm[[column]]
# data_binary <- cbind(data_binary, colnames(data_norm[column]) =  data_norm[[column]])
# data_binary[i] <- data_norm[[column]]
# i <- i + 1
}
}
# install.packages("e1072") # install the package
library(kknn)
# library(MASS)
# library(e1072)
data <- read.table('MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)] #Remove irrelevant data columns
#Remove columns with more than 20% NA
data<-data[, which(colMeans(!is.na(data)) > 0.2)]
#Change NA values with the mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#perform normalization on the data set
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
final_data <- data_norm
# # Real/numerical 2, 35, 36, 37, 38, 84, 86, 87, 88, 89. 90, 91  normalizen, anders te veel effect
# # Ordinal 4 -7, 9, 11 , 45 - 48, 92-95, 100-105
#
# #
# # # Rest Nominal
# # #Classification
# set.seed(2022)
# m <- nrow(final_data)
# ind <- sample(1:m, m/3, prob = rep(1/m,m))
# final_data.train <- final_data[-ind,]
# final_data.test <- final_data[ind,]
# prop.table(table(final_data.train$V122))*100
#
# final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
# fit <- fitted(final_data.knn)
# table(final_data.test$V122, fit)
setwd("C:/Users/20182102/OneDrive - TU Eindhoven/Universiteit/IST/SMDM")
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
final_data <- data_norm
# # Real/numerical 2, 35, 36, 37, 38, 84, 86, 87, 88, 89. 90, 91  normalizen, anders te veel effect
# # Ordinal 4 -7, 9, 11 , 45 - 48, 92-95, 100-105
#
# #
# # # Rest Nominal
# # #Classification
# set.seed(2022)
# m <- nrow(final_data)
# ind <- sample(1:m, m/3, prob = rep(1/m,m))
# final_data.train <- final_data[-ind,]
# final_data.test <- final_data[ind,]
# prop.table(table(final_data.train$V122))*100
#
# final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
# fit <- fitted(final_data.knn)
# table(final_data.test$V122, fit)
# install.packages("e1072") # install the package
library(kknn)
# library(MASS)
# library(e1072)
data <- read.table('MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)] #Remove irrelevant data columns
#Remove columns with more than 20% NA
data<-data[, which(colMeans(!is.na(data)) > 0.2)]
#Change NA values with the mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#perform normalization on the data set
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
final_data <- data_norm
# # Real/numerical 2, 35, 36, 37, 38, 84, 86, 87, 88, 89. 90, 91  normalizen, anders te veel effect
# # Ordinal 4 -7, 9, 11 , 45 - 48, 92-95, 100-105
#
# #
# # # Rest Nominal
# # #Classification
# set.seed(2022)
# m <- nrow(final_data)
# ind <- sample(1:m, m/3, prob = rep(1/m,m))
# final_data.train <- final_data[-ind,]
# final_data.test <- final_data[ind,]
# prop.table(table(final_data.train$V122))*100
#
# final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
# fit <- fitted(final_data.knn)
# table(final_data.test$V122, fit)
View(final_data)
View(final_data)
for (column in final_data){
print(unique(column))
# if (unique(column)== 2){
#
# }
}
for (column in final_data){
print(length(unique(column)))
# if (unique(column)== 2){
#
# }
}
binary_data[colnames(final_data[i])]
binary_data[colnames(final_data[i])]
binary_data[colnames(final_data[i])]
for (i in length(final_data)){
if (length(unique(final_data[i])== 2)){
binary_data[colnames(final_data[i])] <- final_data[i]
}
}
binary_data <- data.frame()
real_data <- data.drame()
for (i in length(final_data)){
if (length(unique(final_data[i])== 2)){
binary_data[colnames(final_data[i])] <- final_data[i]
}
}
colnames(final_data)
colnames(final_data[1])
for (i in length(final_data)){
if (length(unique(final_data[i])== 2)){
binary_data[colnames(final_data[i])] <- final_data[colnames(final_data[i])]
}
}
for (i in length(final_data)){
if (length(unique(final_data[i])== 2)){
binary_data$colnames(final_data[i]) <- final_data[colnames(final_data[i])]
}
}
Filter(function(x) all(x %in% c(0, 1)), final_data)
binary_data <- Filter(function(x) all(x %in% c(0, 1)), final_data)
View(binary_data)
real_data <- Filter(function(x) all(x %!in% colnames(binary_data)), final_data)
real_data <- Filter(function(x) all!(x %in% colnames(binary_data)), final_data)
real_data <- final_data[, -which(names(binary_data) %in% names(binary_data))]
View(real_data)
unique(final_data$V83)
View(binary_data)
final_data <- data_norm
binary_data <- Filter(function(x) all(x %in% c(0, 1)), final_data)
real_data <- as.Dataframe(final_data[, -which(names(binary_data) %in% names(binary_data))])
real_data <- data.frame(final_data[, -which(names(binary_data) %in% names(binary_data))])
View(real_data)
# install.packages("e1072") # install the package
library(kknn)
# library(MASS)
# library(e1072)
data <- read.table('MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)] #Remove irrelevant data columns
#Remove columns with more than 20% NA
data<-data[, which(colMeans(!is.na(data)) > 0.2)]
#Change NA values with the mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#perform normalization on the data set
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
final_data <- data_norm
binary_data <- Filter(function(x) all(x %in% c(0, 1)), final_data)
real_data <- data.frame(final_data[, -which(names(binary_data) %in% names(binary_data))])
# # Real/numerical 2, 35, 36, 37, 38, 84, 86, 87, 88, 89. 90, 91  normalizen, anders te veel effect
# # Ordinal 4 -7, 9, 11 , 45 - 48, 92-95, 100-105
#
# #
# # # Rest Nominal
# # #Classification
# set.seed(2022)
# m <- nrow(final_data)
# ind <- sample(1:m, m/3, prob = rep(1/m,m))
# final_data.train <- final_data[-ind,]
# final_data.test <- final_data[ind,]
# prop.table(table(final_data.train$V122))*100
#
# final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
# fit <- fitted(final_data.knn)
# table(final_data.test$V122, fit)
View(real_data)
real_data <- data.frame(final_data[, -which(names(binary_data) %in% names(final_data))])
View(real_data)
names(final_data)
names(binary_data) %in% names(final_data)
real_data <- data.frame(final_data[, -which(names(final_data) %in% names(binary_data))])
which(names(final_data) %in% names(binary_data))
real_data <- data.frame(final_data[, -which(names(final_data) %in% names(binary_data))])
View(real_data)
data_merged <- binary_data + real_data
data_merged <- merge(binary_data,real_data)
data_merged <- rbind(binary_data,real_data)
data_merged <- rbind.data.frame(binary_data,real_data)
data_merged <- cbind(binary_data,real_data)
#
# #
# # Rest Nominal
# #Classification
set.seed(2022)
m <- nrow(final_data)
ind <- sample(1:m, m/3, prob = rep(1/m,m))
final_data.train <- final_data[-ind,]
final_data.test <- final_data[ind,]
prop.table(table(final_data.train$V122))*100
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
fit
table(final_data.test$V122, fit)
fit
final_data.knn
View(final_data.knn)
unique(final_data$V122)
# # Rest Nominal
# #Classification
set.seed(2022)
m <- nrow(final_data)
ind <- sample(1:m, m/3, prob = rep(1/m,m))
final_data.train <- final_data[-ind,]
final_data.test <- final_data[ind,]
prop.table(table(final_data.train$V122))*100
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 1)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
fit
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 2)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 2, scale = TRUE)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 7, distance = 2, scale = FALSE)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V83~.), train = final_data.train, test = final_data.test, k = 7, distance = 2)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
# # Rest Nominal
# #Classification
set.seed(2022)
m <- nrow(final_data)
ind <- sample(1:m, m/3, prob = rep(1/m,m))
final_data.train <- final_data[-ind,]
final_data.test <- final_data[ind,]
prop.table(table(final_data.train$V122))*100
final_data.knn <- kknn(formula = formula(V83~.), train = final_data.train, test = final_data.test, k = 30, distance = 2)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V83~.), train = final_data.train, test = final_data.test, k = 1, distance = 2)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V83~.), train = final_data.train, test = final_data.test, k = 2, distance = 2)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V83~.), train = final_data.train, test = final_data.test, k = 1, distance = 1)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 1, distance = 1)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
fit
setwd("C:/Users/20182102/OneDrive - TU Eindhoven/Universiteit/IST/SMDM")
# install.packages("e1072") # install the package
library(kknn)
# library(MASS)
# library(e1072)
data <- read.table('MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)] #Remove irrelevant data columns
#Remove columns with more than 20% NA
data<-data[, which(colMeans(!is.na(data)) > 0.2)]
#Change NA values with the mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#perform normalization on the data set
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
final_data <- data_norm
# binary_data <- Filter(function(x) all(x %in% c(0, 1)), final_data)
# real_data <- data.frame(final_data[, -which(names(final_data) %in% names(binary_data))])
# data_merged <- cbind(binary_data,real_data) # merge with cbind
#
# # Real/numerical 2, 35, 36, 37, 38, 84, 86, 87, 88, 89. 90, 91  normalizen, anders te veel effect
# # Ordinal 4 -7, 9, 11 , 45 - 48, 92-95, 100-105
#
# #
# # Rest Nominal
# #Classification
set.seed(2022)
m <- nrow(final_data)
ind <- sample(1:m, m/3, prob = rep(1/m,m))
final_data.train <- final_data[-ind,]
final_data.test <- final_data[ind,]
prop.table(table(final_data.train$V122))*100
final_data.knn <- kknn(formula = formula(V122~.), train = final_data.train, test = final_data.test, k = 1, distance = 1)
fit <- fitted(final_data.knn)
table(final_data.test$V122, fit)
predict(fit)
setwd("~/Github/SMDM")
#Steps in order to fix the dataset
#Load Dataset
# data<-read.csv("datasets/MI.data")
data <- read.table('datasets/MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
#Remove variables that are useless
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)]
#Remove variables that have more than 25% missing values
data<-data[, which(colMeans(!is.na(data)) > 0.25)]
#Fix NaN values: Real variables: Mean, Binary variables: Mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#Normalization/standardization of real values
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
#Split the data set in two seperate dataframes based on Binary vs real/ordinal
binary_data <- Filter(function(x) all(x %in% c(0, 1)), data_norm)
real_data <- data.frame(data_norm[, -which(names(data_norm) %in% names(binary_data))])
#Remove variables with correlation matrix (real variables and ordinal data)
#Remove variables with Chi-Squared test (Binary variables)
#Code for chi-squared correlations between dependent variable and the independent variables
#H0: The two variables are independent.
#H1: The two variables relate to each other.
x <- 1:length(binary_data)
result <- vector('list', length(x))
for(i in x){
test <- chisq.test(binary_data[,colnames(binary_data[i])], y$V122)
result[[i]] <- data.frame("X" = colnames(binary_data[i]),
"Y" = colnames(y),
"Chi.Square" = round(test$statistic,3),
"df"= test$parameter,
"p.value" = round(test$p.value, 3))
}
#print(result) # to see the p values
#p values < 0.05: reject Null hypothesis : the selected variables are dependent, so can be used to predict the dependent variable
#Feature selection: Selection variables that can be removed
x <- 1:length(binary_data)
Remove_vars <- c(rep(0,length(binary_data)))
for(i in x){
if(result[[i]][[5]]> 0.05){
Remove_vars[i] <- result[[i]][[1]]
}
}
#Remove binary variables that have p-value > 0.05
data_final_binary <- binary_data[ , !(names(binary_data) %in% Remove_vars)]
#Optional
#One hot encoding of Ordinal values
#
View(binary_data)
#Steps in order to fix the dataset
#Load Dataset
# data<-read.csv("datasets/MI.data")
data <- read.table('datasets/MI.data', sep = ',')
data[]<-lapply(data, function(x) as.numeric(as.character(x))) #Force all datatypes to be numeric
y <- data[c(122)]#Output
#Remove variables that are useless
data <- data[-c(1, 95, 102, 105, 112:121, 123, 124)]
#Remove variables that have more than 25% missing values
data<-data[, which(colMeans(!is.na(data)) > 0.25)]
#Fix NaN values: Real variables: Mean, Binary variables: Mode
getmode <- function(v) { #Function for the mode
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
data_na <- data
for(i in 1:ncol(data)) {
if (is.na(getmode (data_na[ , i]))){ #If mode is NA, most likely that it is real value, mean is okay
data_na[ , i][is.na(data_na[ , i])] <- mean(data_na[ , i], na.rm = TRUE)
}
else{ #Take for the ordinal and binary
data_na[ , i][is.na(data_na[ , i])] <- getmode(data_na[ , i])
}
}
#Normalization/standardization of real values
min_max_norm <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
data_norm <- as.data.frame(lapply(data_na, min_max_norm))
#Split the data set in two seperate dataframes based on Binary vs real/ordinal
binary_data <- Filter(function(x) all(x %in% c(0, 1)), data_norm)
real_data <- data.frame(data_norm[, -which(names(data_norm) %in% names(binary_data))])
#Remove variables with correlation matrix (real variables and ordinal data)
#Remove variables with Chi-Squared test (Binary variables)
#Code for chi-squared correlations between dependent variable and the independent variables
#H0: The two variables are independent.
#H1: The two variables relate to each other.
x <- 1:length(binary_data)
result <- vector('list', length(x))
for(i in x){
test <- chisq.test(binary_data[,colnames(binary_data[i])], y$V122)
result[[i]] <- data.frame("X" = colnames(binary_data[i]),
"Y" = colnames(y),
"Chi.Square" = round(test$statistic,3),
"df"= test$parameter,
"p.value" = round(test$p.value, 3))
}
#print(result) # to see the p values
#p values < 0.05: reject Null hypothesis : the selected variables are dependent, so can be used to predict the dependent variable
#Feature selection: Selection variables that can be removed
x <- 1:length(binary_data)
Remove_vars <- c(rep(0,length(binary_data)))
for(i in x){
if(result[[i]][[5]]> 0.05){
Remove_vars[i] <- result[[i]][[1]]
}
}
#Remove binary variables that have p-value > 0.05
data_final_binary <- binary_data[ , !(names(binary_data) %in% Remove_vars)]
#Removal of redundant variables real and ordinal data
#Get correlation matrix. Don't forget to ignore categorical variables in this process
cormat<-round(cor(real_data),2)
################### AUXILIARY FUNCTIONS ##########################
# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
cormat[lower.tri(cormat)]<- NA
return(cormat)
}
##################################################################
upper_tri <- get_upper_tri(cormat)
#Remove one variable of pair with high correlation.
related_index = c()
for (line in 1:dim(upper_tri)[1]){
for (col in 1:line){
i = col+dim(upper_tri)[1]*(line-1)
if (abs(upper_tri[i])>0.8 & abs(upper_tri[i])<1){
related_index<-c(related_index, line)
}
}
}
data_final_real<-real_data[-related_index]
#Merge
final_data <- cbind(data_final_binary,data_final_real)
#Optional
#One hot encoding of Ordinal values
#
View(final_data)
View(final_data)
min(y)
max(y)
